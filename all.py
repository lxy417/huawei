#使用此文件前打开bert-serving-start  -model_dir E:/model -num_worker=2
from bert_serving.client import BertClient
import pickle
from sklearn import linear_model
from sklearn import svm
from sklearn.manifold import TSNE
import math
import numpy as np
np.random.seed(71)
from keras import backend as K
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras.callbacks import Callback
from keras.utils import np_utils
from keras.objectives import categorical_crossentropy
from wxpy import *
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
import multiprocessing as mp
batch_size = 10
low_dim = 2
nb_epoch = 100
shuffle_interval = nb_epoch + 1
n_jobs = 4
perplexity = 30.0
channel = 1
row=1
col=768
OCsvm=svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)
ll=linear_model.LogisticRegression()
model = Sequential()

def Hbeta(D, beta):
    P = np.exp(-D * beta)
    sumP = np.sum(P)
    H = np.log(sumP) + beta * np.sum(D * P) / sumP
    P = P / sumP
    return H, P
def x2p_job(data):
    i, Di, tol, logU = data
    beta = 1.0
    betamin = -np.inf
    betamax = np.inf
    H, thisP = Hbeta(Di, beta)

    Hdiff = H - logU
    tries = 0
    while np.abs(Hdiff) > tol and tries < 50:
        if Hdiff > 0:
            betamin = beta
            if betamax == -np.inf:
                beta = beta * 2
            else:
                beta = (betamin + betamax) / 2
        else:
            betamax = beta
            if betamin == -np.inf:
                beta = beta / 2
            else:
                beta = (betamin + betamax) / 2

        H, thisP = Hbeta(Di, beta)
        Hdiff = H - logU
        tries += 1

    return i, thisP
def x2p(X):
    tol = 1e-5
    n = X.shape[0]
    logU = np.log(perplexity)

    sum_X = np.sum(np.square(X), axis=1)
    D = sum_X + (sum_X.reshape([-1, 1]) - 2 * np.dot(X, X.T))

    idx = (1 - np.eye(n)).astype(bool)
    D = D[idx].reshape([n, -1])

    def generator():
        for i in range(n):
            yield i, D[i], tol, logU

    pool = mp.Pool(n_jobs)
    result = pool.map(x2p_job, generator())
    P = np.zeros([n, n])
    for i, thisP in result:
        P[i, idx[i]] = thisP

    return P
def calculate_P(X):
    print ("Computing pairwise distances...")
    n = X.shape[0]
    P = np.zeros([n, batch_size])
    for i in range(0, n, batch_size):
        P_batch = x2p(X[i:i + batch_size])
        P_batch[np.isnan(P_batch)] = 0
        P_batch = P_batch + P_batch.T
        P_batch = P_batch / P_batch.sum()
        P_batch = np.maximum(P_batch, 1e-12)
        P[i:i + batch_size] = P_batch
    return P
def KLdivergence(P, Y):
    alpha = low_dim - 1.
    sum_Y = K.sum(K.square(Y), axis=1)
    eps = K.variable(10e-15)
    D = sum_Y + K.reshape(sum_Y, [-1, 1]) - 2 * K.dot(Y, K.transpose(Y))
    Q = K.pow(1 + D / alpha, -(alpha + 1) / 2)
    Q *= K.variable(1 - np.eye(batch_size))
    Q /= K.sum(Q)
    Q = K.maximum(Q, eps)
    C = K.log((P + eps) / (Q + eps))
    C = K.sum(P * C)
    return C




#加载tsne模型


print ("build model")
model.add(Dense(500, input_shape=(768,)))
model.add(Activation('relu'))
model.add(Dense(500))
model.add(Activation('relu'))
model.add(Dense(2000))
model.add(Activation('relu'))
model.add(Dense(2))
model.compile(loss=KLdivergence, optimizer="adam")
model.load_weights(r"E:\xingyu\test\TSNE.h5")
    # tsne=TSNE()

    #加载分类模型
with open(r'E:\xingyu\test\logistic_4_91.pickle.dat', 'rb') as f:
         ll = pickle.load(f)

    # with open(r'/home/shandian_ll/HuaweiCup/zhiqiang/oneclass/jw_437.pickle.dat', 'rb') as f:
    #      tsne = pickle.load(f)

    #加载ocsvm

with open(r'E:\xingyu\test\test_491.pickle.dat','rb') as f:
         OCSvm=pickle.load(f)

def shuchu(sen,ll,OCSvm1,model):
    bc = BertClient()
    # print("请输入一句话")
    # sen=input()
    a=bc.encode([sen])   #a是句向量
    # a=[[1.2353455,-0.60615593,1.1676943,1.0952734,0.5405869,0.4267459,-0.03270419,-0.16009042,0.283612,0.03223125,-0.033010013,-0.7579848,-0.5671608,0.3845738,0.42658952,-0.82196885,0.5073282,0.17894463,0.63128364,1.0591208,0.33271894,0.35259137,0.087158084,0.82725096,0.7381821,0.4708064,-0.23917593,-0.14888988,-0.23613581,0.45533687,0.6370357,0.10094999,-0.89609814,-0.008540995,1.1413186,0.16875446,0.6387194,-0.30624264,-0.34916636,-0.0059698075,-0.06404201,0.07515635,0.27891597,-1.0358276,-0.05870723,0.46868318,-0.13060942,0.30375072,0.062006876,0.3798707,-0.94306904,5.5089035,0.18927695,0.54202676,0.15754482,0.7569257,-0.55725163,0.06457002,0.06239201,0.8228175,0.49799737,0.4382023,1.0482931,-0.20235954,0.19845378,0.055630747,0.22424868,-0.41472456,0.39769372,-0.0974882,0.93304574,-0.56177056,-0.4311078,-0.34411582,0.27857518,1.2426431,-1.1320244,1.0405389,0.37894848,-0.17461896,0.0706592,-0.32954365,-0.7773271,-0.7506839,-0.6469703,-0.35738793,-0.26882684,0.34681192,-0.3685746,0.5949221,-0.22034474,0.6260466,0.066296175,0.2577441,0.5076558,0.03793086,-0.80271804,0.15085787,-0.91034365,0.2553727,1.132261,0.4051871,-0.23803502,-1.0981947,0.3334135,1.4298558,0.089089416,0.7731308,1.21085,1.2135817,-0.34975788,-0.49217302,-0.11358845,-0.025138201,0.29441407,-0.58637935,0.2712038,-0.42190462,1.3556539,0.3593957,-0.5336413,-0.3361788,-1.5776762,-0.59065074,-0.073861204,-0.20169403,-1.4411579,0.69937223,0.53842294,-0.18878016,0.1175386,0.13475968,1.1204741,0.21493945,0.0665362,-0.77215314,-0.3103683,-0.5901548,-0.49172282,-0.18555845,-0.7962291,-0.31582785,-1.2497802,-0.3395838,0.40000907,0.15380423,-0.17972803,-0.019075897,-0.34163988,-0.24319495,0.6564579,0.037661113,1.3465306,0.6132049,0.64816517,-0.4617881,-0.0012711468,1.183452,0.42089796,-0.23882599,-0.8160244,-0.34228423,0.6631982,-1.036569,-0.7086513,-1.1328143,0.4506308,0.6517082,-0.6979378,0.34766886,0.5805705,-0.13675499,0.09059925,-0.87029886,1.1527258,0.33888212,-0.5300097,-0.39820892,-0.7710195,1.0729697,-0.27009207,-0.46284786,-0.7693618,-0.36022484,-0.16508015,0.37444726,-0.4225031,-0.36471304,0.31182262,0.18250817,0.094440706,-0.36750498,-0.30984798,-0.7605639,-0.47757304,-1.517367,-0.07551893,0.4623681,-0.2671159,-0.58784866,0.1262791,-0.2883541,-0.8215848,-0.59506786,-0.73973423,1.1149312,0.2931516,-0.678015,-0.50457996,0.21689351,-0.6726612,-0.505727,-0.060522787,0.62086976,-1.3800412,-0.10567729,0.28592384,0.5129704,-0.6706777,0.32656366,-0.60672116,-0.7403129,0.35207725,0.049254537,0.083804294,0.23851378,-0.14317861,-0.22231959,0.84582156,1.0010011,-0.24827914,-0.4671666,-0.66018075,0.006898878,0.7047916,0.14782579,-0.24093327,-0.4452185,-0.6614543,0.925946,0.8960943,0.41995582,0.039212458,-1.165004,-0.7390153,0.61909646,-0.42644078,-0.11818777,-0.48487392,0.12299155,-1.0480733,-0.23166353,-0.24385032,0.53863585,0.44046947,0.3537907,-1.0700107,0.69470954,0.5375739,0.1053835,-0.25426677,0.3915988,-0.8744681,0.5061297,-0.7030523,1.0201494,0.0974469,-0.008315575,0.29359302,-0.8477209,-0.023998288,-0.16468132,-0.5183167,-0.3935745,0.057474468,0.4862095,-1.3116934,0.4330711,0.043980416,-0.45383322,-0.85980165,0.41630748,0.7582423,-0.61922026,-1.1014458,-0.47698125,0.44779545,-0.39442334,1.0361005,0.723102,-1.1553053,-0.62879497,0.043538753,-0.09355337,1.8642607,-0.83193076,-0.1101239,-0.689503,-0.4025172,0.21091709,-0.237554,0.047531832,-0.07761395,0.407452,0.06604867,-0.44256553,0.8927733,-0.31457198,-0.7065479,-0.81530464,-0.22912057,-0.09354338,-0.04549536,0.42389807,0.19421984,0.5147136,-0.1735222,0.1823817,0.45486826,0.19569768,-0.8015532,-0.53253275,-0.07365543,1.0703295,0.53944016,1.2866907,1.5749196,0.4631979,-0.12789601,1.0364698,-1.3966287,0.27336025,0.16679451,-0.41650304,0.36528555,-0.6768507,-0.5733202,-0.8603396,-0.52805394,0.3715569,0.27321327,1.4650402,-0.37788033,-0.06176519,-0.4551045,0.5957352,1.1985552,-0.26173717,-0.47323725,-0.4724319,-0.6161051,-1.2417896,0.30468738,-0.22477002,-0.084866226,-0.48814458,-0.35215813,0.3113639,0.46658292,0.18643375,0.45128313,-0.44258416,-0.39326835,-0.8488216,-0.075453244,-0.10653422,0.15825444,0.2154736,0.08486833,-0.8519193,-0.17723364,0.39689538,-0.48756662,0.15056945,-1.135072,0.39844516,0.88012654,0.37327287,-0.2067266,0.2490423,-1.5705073,0.8824359,0.48403686,-0.32656926,0.48647052,0.7150039,-0.3377111,0.24406335,0.012907216,-0.52434754,0.041781344,-0.03932303,0.43594736,0.40757594,0.97239125,0.06592637,0.12809935,-1.0660434,-0.53224653,0.113692686,1.716986,0.013540032,-0.1961073,-0.275878,0.05991446,-0.26575506,-0.8302739,0.5688971,0.0038376648,0.06931498,-0.65034837,0.66335505,0.7447493,1.7902746,0.6764877,-0.24024847,1.0518689,-0.9177509,0.5284828,-0.48330417,0.57253927,0.43919814,0.0018989353,0.54110074,-0.08995094,0.41811296,-0.80185366,0.7777329,1.4792726,-0.07405012,0.55963916,0.4610984,0.02373079,-0.31538326,0.51255083,0.05521285,0.4542328,-1.1357813,1.1551851,-0.6581021,0.24608153,-0.5118465,-0.3368461,-0.2080771,-0.24233371,0.04594263,0.08238606,-0.44755933,0.09900667,0.0070696394,-0.64612913,-0.46362457,-1.1963357,-0.67582273,0.36939844,0.09922932,-0.9158949,1.34302,-0.632243,0.18020977,-0.50356907,-0.2186588,-0.6198025,-0.97094357,-0.08564533,-0.0031866338,0.51040566,0.6155821,0.59427863,-0.108451344,0.53741956,0.27663314,0.88311154,0.38750196,-0.34633267,-0.4072059,0.32247004,-0.13625684,-0.25505188,0.290936,1.0252322,0.9066264,-0.15924947,0.30955172,-0.22169538,0.019868772,0.57101053,-0.8689907,0.6638548,-0.11305906,-1.3664554,0.31149256,1.2872926,-0.22719161,0.17831479,-0.029571535,0.9041845,0.0015869401,0.57055414,0.5663592,0.55709887,-0.27147755,0.6282016,0.3987571,0.29771617,0.07680502,-0.29250228,-1.0925521,0.2871119,0.36214024,-0.09029315,-0.6920369,0.52715755,0.033012558,-0.5180682,-0.025180548,-0.20394996,0.5948774,-0.024000866,-0.11702414,0.10498037,0.41316587,-0.2776855,-0.21594709,-1.0131048,0.17387182,0.5900109,-0.59703434,0.81912833,0.30781522,0.026221748,-1.179072,0.38408092,-0.5324838,0.5199857,0.25952974,0.02228786,-0.29426855,-0.5800365,0.58228606,-0.0033356997,-0.07626499,-0.049246423,1.2795618,0.58532107,-0.54789895,-0.8067627,0.04917367,-0.27760035,0.25926328,0.12947959,0.32204944,-0.096452735,0.97194844,0.8076024,0.36123684,-0.059489,-0.09326875,1.2382032,-0.5407102,0.4451103,-0.6673128,-1.3754133,0.31496888,-0.32509923,0.8657216,-1.0075587,1.0361935,1.3070319,0.10084186,-0.5545229,0.28643358,0.885482,0.35566646,-0.7086807,0.36201784,0.404539,-0.20577896,1.5079058,-0.2560911,0.95958114,0.48898065,0.8270787,0.41020304,0.40211433,-0.13539809,0.27457136,0.31774008,0.17174338,-0.60698336,-0.2620931,0.61339295,0.99565756,-0.10578776,0.1939182,0.48949072,0.03619484,-0.31101814,0.5219242,0.28172255,-0.606573,-0.3863992,0.122780375,0.43927535,-0.3738001,0.12930189,-0.13013653,-0.3799662,-0.27555075,0.0028320497,-0.43891343,0.6785218,0.46259192,-0.074753165,-0.52070534,1.2359308,1.1463938,-0.00844935,-0.21644072,0.08047881,-0.9597448,-1.0734377,0.48593664,-0.13388392,0.99492383,0.91817987,1.5050272,-0.44783476,0.17227559,-0.57377076,-0.18692556,-0.24249065,0.29941374,-1.0012494,0.6077528,0.38924405,0.5855457,0.79171765,0.8344568,-1.1681595,0.4315655,-1.1060324,-0.11117511,0.21705431,-0.5657374,-0.10054155,-0.122824416,0.47883928,-0.65339476,-0.309827,0.49640915,0.9608143,0.7935139,-0.5453001,0.6839435,-0.6652574,0.13321291,1.0453342,-0.19726892,0.04662127,-0.44378218,-0.18878579,0.31383973,0.18584128,0.05956708,-0.49912515,-0.620415,0.9373734,0.36046842,0.40758172,0.61921346,0.83168924,-0.8464507,1.1985561,0.3915953,-0.18563923,-0.43159446,-0.14361912,0.43120864,-0.92157763,0.7842031,0.7288936,-0.37303683,0.4842522,-0.2606066,0.12722534,-0.5623275,-0.2471412,-0.7141172,-0.95333785,0.35184845,-0.43976495,-0.085216,-0.66137797,0.54005426,-8.602665,-0.60200727,-0.66189486,-0.48353207,-0.7628243,0.44049963,0.4394273,0.43878174,0.9905425,0.22037688,0.9808238,0.7690122,-0.106913485,-0.16246387,-0.25884068,-0.8171177,-0.32393244,-0.38756558,-0.00012859539,0.30207843,0.26675314,0.19402505,-0.36489913,-0.15804291,0.8427812,0.05974044,-0.025579479,0.45148462,0.6725323,-0.07506579,-0.15865232,-0.8080466,-0.035792608,0.14472629,-0.97396415,0.03289976,-0.30919233,-0.4915681,0.04439352,-0.35519627,-0.42409447,-0.76235527,0.029973317,0.28637648,-0.29177237,-0.7156375,0.0066093984,0.5309505,1.2372495,-0.8740382,0.5243783,-0.048716616,-0.79093647,-0.60997844,-0.66231793,-0.18666828,0.1880434,-0.59915996,-1.1414096,-1.2251972,-0.88055986,-0.14110534,-0.3858317,0.77120256,1.1734672,-0.22278783,-0.92202497,0.40079942,0.059390094,0.18108329,0.56651616,0.3752802,0.60230803]]
    a=np.array(a)
    #aa为预测的概率，sum为熵
    X_test = a.astype('float32')
    aa=ll.predict_proba(a)
    sum=0
    for j in aa[0]:
        sum=sum-j*math.log(j,2)


    #df是降维后的数据，df1是合并后的数据
    # df=tsne.fit_transform(a)
    df=model.predict(X_test)
    df1=df[0].tolist()
    df1.append(sum)
    df1=np.array([df1])
    #ocsvm是检测域内域外
    #pre为-1是域外，1是域内
    pre=OCSvm1.predict(df1)

    #pre=[1]
    if(pre[0]==1):
        #预测类别
        pre_1=ll.predict(a)
        #pre_1是类别
        return pre_1
    else:
        return "域外数据"
def aaa():
   print(shuchu("aaa",ll,OCSvm,model))
aaa()
ret = shuchu("aa",ll,OCSvm,model)
print(ret[0])
bot = Bot(console_qr=False, cache_path=False)
bot.file_helper.send('hello,world')
#search里面加用户
found = bot.friends().search('冉开怀')
@bot.register(found)
def forward_message(msg):
    print(msg.text)
    # print( print('[接收]' + str(msg)))
    # return shuchu(msg.text)
    aaa=msg.text
    ret = shuchu(aaa,ll,OCSvm,model)
    print(ret)
    return ret
embed()